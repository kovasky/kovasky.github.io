<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on Kovasky Buezo | Software Developer, Cybersecurity Enthusiast</title><link>https://kovasky.me/tags/ai/</link><description>Recent content in Ai on Kovasky Buezo | Software Developer, Cybersecurity Enthusiast</description><generator>Hugo</generator><language>en</language><lastBuildDate>Fri, 21 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://kovasky.me/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Build and run ollama with vulkan support to enable local inference on Intel Arc GPUs</title><link>https://kovasky.me/blogs/ollama_vulkan_intel/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://kovasky.me/blogs/ollama_vulkan_intel/</guid><description>&lt;h2 id="intro"&gt;Intro&lt;/h2&gt;
&lt;p&gt;I have an Intel Arc A380 in my Dell T440 server for Plex and remote gaming through Sunshine. I set up &lt;a href="https://hoarder.app"&gt;Hoarder&lt;/a&gt; and wanted a way to use the AI features without paying for OpenAI tokens. This proved extremely difficult, as none of the readily available solutions seemed to work.&lt;/p&gt;
&lt;p&gt;This was until I tried LMStudio. LMStudio worked out of the box, but launching it in a headless manner on boot is not possible (at least for me), not to mention the incompatibility of its OpenAPI implementation with Hoarder. I later learned that LMStudio uses a vulkan backend, and went on a quest to find an ollama version with vulkan support.&lt;/p&gt;</description></item></channel></rss>