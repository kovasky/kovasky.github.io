<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ollama on Kovasky Buezo | Desarrollador de software, entusiasta de la ciberseguridad</title><link>http://localhost:1313/es/tags/ollama/</link><description>Recent content in Ollama on Kovasky Buezo | Desarrollador de software, entusiasta de la ciberseguridad</description><generator>Hugo</generator><language>es</language><lastBuildDate>Fri, 21 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/es/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>Build ollama with vulkan support to enable local inference on Intel Arc GPUs</title><link>http://localhost:1313/es/blogs/ollama_vulkan_intel/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/es/blogs/ollama_vulkan_intel/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>I have an Intel Arc A380 in my Dell T440 server for Plex and remote gaming through Sunshine. I set up &lt;a href="https://hoarder.app">Hoarder&lt;/a> and wanted a way to use the AI features without paying for OpenAI tokens. This proved extremely difficult, as none of the readily available solutions seemed to work.&lt;/p>
&lt;p>This was until I tried LMStudio. LMStudio worked out of the box, but launching it in a headless manner on boot is not possible (at least for me), not to mention the incompatibility of its OpenAPI implementation with Hoarder. I later learned that LMStudio uses a vulkan backend, and went on a quest to find an ollama version with vulkan support.&lt;/p></description></item></channel></rss>